<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Home · MachineLearningCourse.jl</title><meta name="title" content="Home · MachineLearningCourse.jl"/><meta property="og:title" content="Home · MachineLearningCourse.jl"/><meta property="twitter:title" content="Home · MachineLearningCourse.jl"/><meta name="description" content="Documentation for MachineLearningCourse.jl."/><meta property="og:description" content="Documentation for MachineLearningCourse.jl."/><meta property="twitter:description" content="Documentation for MachineLearningCourse.jl."/><meta property="og:url" content="https://rajgoel.github.io/course-machine-learning/julia/"/><meta property="twitter:url" content="https://rajgoel.github.io/course-machine-learning/julia/"/><link rel="canonical" href="https://rajgoel.github.io/course-machine-learning/julia/"/><script data-outdated-warner src="assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="search_index.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href>MachineLearningCourse.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li class="is-active"><a class="tocitem" href>Home</a><ul class="internal"><li><a class="tocitem" href="#Installation"><span>Installation</span></a></li><li><a class="tocitem" href="#Usage"><span>Usage</span></a></li><li><a class="tocitem" href="#Course-material"><span>Course material</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Home</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Home</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/rajgoel/course-machine-learning" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/rajgoel/course-machine-learning/blob/main/julia/docs/src/index.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="MachineLearningCourse.jl"><a class="docs-heading-anchor" href="#MachineLearningCourse.jl">MachineLearningCourse.jl</a><a id="MachineLearningCourse.jl-1"></a><a class="docs-heading-anchor-permalink" href="#MachineLearningCourse.jl" title="Permalink"></a></h1><p>A Julia package for machine learning course materials and implementations.</p><h2 id="Installation"><a class="docs-heading-anchor" href="#Installation">Installation</a><a id="Installation-1"></a><a class="docs-heading-anchor-permalink" href="#Installation" title="Permalink"></a></h2><p>The recommended way to use this package is to clone the repository</p><pre><code class="language-bash hljs">git clone https://github.com/rajgoel/course-machine-learning</code></pre><p>and load the module as follows (replace <code>path/to/</code> by the path to the repository):</p><pre><code class="language-julia hljs">using Pkg
Pkg.develop(path=&quot;path/to/course-machine-learning/julia&quot;)</code></pre><p>Alternatively, you can directly load the module as follows:</p><pre><code class="language-julia hljs">using Pkg
Pkg.develop(url=&quot;https://github.com/rajgoel/course-machine-learning&quot;, subdir=&quot;julia&quot;)</code></pre><h2 id="Usage"><a class="docs-heading-anchor" href="#Usage">Usage</a><a id="Usage-1"></a><a class="docs-heading-anchor-permalink" href="#Usage" title="Permalink"></a></h2><p>To use the module type:</p><pre><code class="language-julia hljs">using MachineLearningCourse</code></pre><p>To access specific lecture modules (replace <code>XX</code> with the two-digit lecture number):</p><pre><code class="language-julia hljs">using MachineLearningCourse.LectureXX</code></pre><p>For example, to run the demos:</p><pre><code class="language-julia hljs">using MachineLearningCourse

# Run Lecture 02 gradient descent demo
MachineLearningCourse.Lecture02.demo()

# Run Lecture 03 deep network demo
MachineLearningCourse.Lecture03.demo()</code></pre><h2 id="Course-material"><a class="docs-heading-anchor" href="#Course-material">Course material</a><a id="Course-material-1"></a><a class="docs-heading-anchor-permalink" href="#Course-material" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MachineLearningCourse.MachineLearningCourse" href="#MachineLearningCourse.MachineLearningCourse"><code>MachineLearningCourse.MachineLearningCourse</code></a> — <span class="docstring-category">Module</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><p><strong>MachineLearningCourse</strong></p><p>A Julia package for machine learning course materials and implementations.</p><p><strong>Content</strong></p><ul><li><a href="#MachineLearningCourse.Lecture02"><code>Lecture02</code></a>: Gradient descent</li><li><a href="#MachineLearningCourse.Lecture03"><code>Lecture03</code></a>: Feed forward networks</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/rajgoel/course-machine-learning/blob/ea8c3bd49b4921d13be396e0667f546d7ad6e697/julia/src/MachineLearningCourse.jl#L1-L10">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MachineLearningCourse.Lecture02" href="#MachineLearningCourse.Lecture02"><code>MachineLearningCourse.Lecture02</code></a> — <span class="docstring-category">Module</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><p><strong>Lecture02: Gradient Descent</strong></p><p>A vanilla implementation of gradient descent.</p><p><strong>Exported Functions</strong></p><ul><li><a href="#MachineLearningCourse.Lecture02.demo"><code>demo</code></a>: 5x5 digit gradient descent demonstration</li></ul><p><strong>Usage Examples</strong></p><p>Create and train a neural network:</p><pre><code class="language-julia hljs">using MachineLearningCourse.Lecture03

# Create network: 2 inputs → 4 hidden → 3 hidden → 1 output
network = DNN([2, 4, 3, 1])

# Train with learning rate 0.1 for 1000 epochs
losses = train!(network, X_train, Y_train, 0.1, 1000)

# Make predictions
prediction = predict(network, x_test)</code></pre><p>Run the 5x5 digit demo:</p><pre><code class="language-julia hljs">demo()</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/rajgoel/course-machine-learning/blob/ea8c3bd49b4921d13be396e0667f546d7ad6e697/julia/02-lecture/Lecture02.jl#L1-L32">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MachineLearningCourse.Lecture02.compute_average_gradients-Tuple{Matrix{Float64}, Vector{Float64}, Vector{Vector{Float64}}, Vector{Vector{Float64}}}" href="#MachineLearningCourse.Lecture02.compute_average_gradients-Tuple{Matrix{Float64}, Vector{Float64}, Vector{Vector{Float64}}, Vector{Vector{Float64}}}"><code>MachineLearningCourse.Lecture02.compute_average_gradients</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">compute_average_gradients(W, b, X, Y)</code></pre><p>Compute average gradients across all training samples for batch gradient descent.</p><p>Performs gradient computation for each sample and averages the results:</p><ul><li>For each sample (x<em>i, y</em>i): compute ∇W<em>i, ∇b</em>i</li><li>Return average: (1/N) * Σ(∇W<em>i), (1/N) * Σ(∇b</em>i)</li></ul><p><strong>Arguments</strong></p><ul><li><code>W::Matrix{Float64}</code>: Weight matrix (n<em>outputs × n</em>inputs)</li><li><code>b::Vector{Float64}</code>: Bias vector (n_outputs,)</li><li><code>X::Vector{Vector{Float64}}</code>: Training input data (N samples)</li><li><code>Y::Vector{Vector{Float64}}</code>: Training target data (N samples)</li></ul><p><strong>Returns</strong></p><ul><li><code>Tuple{Matrix{Float64}, Vector{Float64}}</code>: (avg<em>∇W, avg</em>∇b)<ul><li><code>avg_∇W</code>: Average weight gradients</li><li><code>avg_∇b</code>: Average bias gradients</li></ul></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/rajgoel/course-machine-learning/blob/ea8c3bd49b4921d13be396e0667f546d7ad6e697/julia/02-lecture/GradientDescent.jl#L76-L95">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MachineLearningCourse.Lecture02.compute_gradients-Tuple{Matrix{Float64}, Vector{Float64}, Vector{Float64}, Vector{Float64}}" href="#MachineLearningCourse.Lecture02.compute_gradients-Tuple{Matrix{Float64}, Vector{Float64}, Vector{Float64}, Vector{Float64}}"><code>MachineLearningCourse.Lecture02.compute_gradients</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">compute_gradients(W, b, x, y)</code></pre><p>Compute gradients ∂ℒ/∂W and ∂ℒ/∂b for a single sample using backpropagation.</p><p>Calculates gradients using the chain rule:</p><ul><li>∂ℒ/∂W = δ * x^T where δ = ∂ℒ/∂â</li><li>∂ℒ/∂b = δ</li></ul><p><strong>Arguments</strong></p><ul><li><code>W::Matrix{Float64}</code>: Weight matrix (n<em>outputs × n</em>inputs)</li><li><code>b::Vector{Float64}</code>: Bias vector (n_outputs,)</li><li><code>x::Vector{Float64}</code>: Input vector for single sample</li><li><code>y::Vector{Float64}</code>: Target output vector for single sample</li></ul><p><strong>Returns</strong></p><ul><li><code>Tuple{Matrix{Float64}, Vector{Float64}}</code>: (∇W, ∇b)<ul><li><code>∇W</code>: Weight gradients (same size as W)</li><li><code>∇b</code>: Bias gradients (same size as b)</li></ul></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/rajgoel/course-machine-learning/blob/ea8c3bd49b4921d13be396e0667f546d7ad6e697/julia/02-lecture/GradientDescent.jl#L36-L55">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MachineLearningCourse.Lecture02.demo" href="#MachineLearningCourse.Lecture02.demo"><code>MachineLearningCourse.Lecture02.demo</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">demo()</code></pre><p>Demonstration of gradient descent on 5x5 digit recognition.</p><p>Loads training data, initializes a linear classifier, and optimizes parameters using gradient descent. Prints initial and final loss values.</p><p><strong>Implementation Details</strong></p><ul><li>Network: Linear classifier (25 inputs → 10 outputs)</li><li>Loss: Mean Squared Error</li><li>Optimization: Batch gradient descent</li><li>Random initialization for weights and biases</li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs">demo()  # Uses sample data file</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/rajgoel/course-machine-learning/blob/ea8c3bd49b4921d13be396e0667f546d7ad6e697/julia/02-lecture/Demo.jl#L105-L123">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MachineLearningCourse.Lecture02.gradient_descent-Tuple{Matrix{Float64}, Vector{Float64}, Vector{Vector{Float64}}, Vector{Vector{Float64}}}" href="#MachineLearningCourse.Lecture02.gradient_descent-Tuple{Matrix{Float64}, Vector{Float64}, Vector{Vector{Float64}}, Vector{Vector{Float64}}}"><code>MachineLearningCourse.Lecture02.gradient_descent</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">gradient_descent(W_0, b_0, X, Y)</code></pre><p>Optimize neural network parameters using batch gradient descent.</p><p>Implements the complete gradient descent algorithm:</p><ol><li>Initialize parameters from W<em>0, b</em>0</li><li>For each iteration:<ul><li>Compute average gradients across all samples</li><li>Update parameters: W ← W - α * ∇W, b ← b - α * ∇b</li><li>Check convergence using gradient norm</li></ul></li><li>Stop when ‖∇‖ &lt; tolerance or max iterations reached</li></ol><p><strong>Arguments</strong></p><ul><li><code>W_0::Matrix{Float64}</code>: Initial weight matrix (n<em>outputs × n</em>inputs)</li><li><code>b_0::Vector{Float64}</code>: Initial bias vector (n_outputs,)</li><li><code>X::Vector{Vector{Float64}}</code>: Training input data</li><li><code>Y::Vector{Vector{Float64}}</code>: Training target data (one-hot encoded)</li></ul><p><strong>Returns</strong></p><ul><li><code>Tuple{Matrix{Float64}, Vector{Float64}}</code>: (W, b)<ul><li><code>W</code>: Optimized weight matrix</li><li><code>b</code>: Optimized bias vector</li></ul></li></ul><p><strong>Implementation Details</strong></p><ul><li>Learning rate: α = 0.1</li><li>Convergence tolerance: 1e-3</li><li>Maximum iterations: 10,000</li><li>Uses MSE loss function: ℒ = ‖â - y‖²</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/rajgoel/course-machine-learning/blob/ea8c3bd49b4921d13be396e0667f546d7ad6e697/julia/02-lecture/GradientDescent.jl#L135-L164">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MachineLearningCourse.Lecture02.gradient_norm-Tuple{Any, Any}" href="#MachineLearningCourse.Lecture02.gradient_norm-Tuple{Any, Any}"><code>MachineLearningCourse.Lecture02.gradient_norm</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">gradient_norm(∇W, ∇b)</code></pre><p>Compute the Euclidean norm of the combined gradient vector.</p><p>Flattens and concatenates weight and bias gradients into a single vector, then computes ‖∇‖ = √(‖∇W‖² + ‖∇b‖²) for convergence monitoring.</p><p><strong>Arguments</strong></p><ul><li><code>∇W::Matrix{Float64}</code>: Weight gradients</li><li><code>∇b::Vector{Float64}</code>: Bias gradients</li></ul><p><strong>Returns</strong></p><ul><li><code>Float64</code>: Euclidean norm of the combined gradient vector</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/rajgoel/course-machine-learning/blob/ea8c3bd49b4921d13be396e0667f546d7ad6e697/julia/02-lecture/GradientDescent.jl#L110-L124">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MachineLearningCourse.Lecture02.one_hot_encode-Tuple{Int64, Int64}" href="#MachineLearningCourse.Lecture02.one_hot_encode-Tuple{Int64, Int64}"><code>MachineLearningCourse.Lecture02.one_hot_encode</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">one_hot_encode(label, num_classes)</code></pre><p>Convert class labels to one-hot vectors for classification.</p><p><strong>Arguments</strong></p><ul><li><code>label::Int</code>: Class label (1-indexed)</li><li><code>num_classes::Int</code>: Total number of classes</li></ul><p><strong>Returns</strong></p><ul><li><code>Vector{Float64}</code>: One-hot encoded vector</li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs">one_hot_encode(3, 10)  # Returns [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/rajgoel/course-machine-learning/blob/ea8c3bd49b4921d13be396e0667f546d7ad6e697/julia/02-lecture/Demo.jl#L1-L17">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MachineLearningCourse.Lecture02.read_data-Tuple{Any}" href="#MachineLearningCourse.Lecture02.read_data-Tuple{Any}"><code>MachineLearningCourse.Lecture02.read_data</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">read_data(file_path)</code></pre><p>Read 5x5 digit training data from a text file.</p><p>File format: Each digit consists of 6 lines:</p><ul><li>5 lines of 5 space-separated Float64 values (5x5 pixel grid)</li><li>1 line with the digit label (0-9)</li></ul><p><strong>Arguments</strong></p><ul><li><code>file_path::String</code>: Path to the data file</li></ul><p><strong>Returns</strong></p><ul><li><code>Tuple{Vector{Vector{Float64}}, Vector{Vector{Float64}}}</code>: (X, Y)<ul><li><code>X</code>: Input vectors (each vector has 25 elements from 5x5 grid)</li><li><code>Y</code>: One-hot encoded target vectors (10 classes, 1-indexed)</li></ul></li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs">X, Y = read_data(&quot;5x5digits.txt&quot;)
# X[1] contains 25 pixel values for first digit
# Y[1] contains one-hot vector for first digit&#39;s class</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/rajgoel/course-machine-learning/blob/ea8c3bd49b4921d13be396e0667f546d7ad6e697/julia/02-lecture/Demo.jl#L24-L47">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MachineLearningCourse.Lecture02.total_loss-Tuple{Matrix{Float64}, Vector{Float64}, Vector{Vector{Float64}}, Vector{Vector{Float64}}}" href="#MachineLearningCourse.Lecture02.total_loss-Tuple{Matrix{Float64}, Vector{Float64}, Vector{Vector{Float64}}, Vector{Vector{Float64}}}"><code>MachineLearningCourse.Lecture02.total_loss</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">total_loss(W, b, X, Y)</code></pre><p>Compute total Mean Squared Error loss across all training samples.</p><p>For each sample, performs forward pass and computes loss:</p><ul><li>Forward pass: â = W * x + b</li><li>Sample loss: ℒ(y, â) = ‖â - y‖²</li><li>Total loss: Σ ℒ(y<em>i, â</em>i) over all samples</li></ul><p><strong>Arguments</strong></p><ul><li><code>W::Matrix{Float64}</code>: Weight matrix (n<em>outputs × n</em>inputs)</li><li><code>b::Vector{Float64}</code>: Bias vector (n_outputs,)</li><li><code>X::Vector{Vector{Float64}}</code>: Training input data</li><li><code>Y::Vector{Vector{Float64}}</code>: Training target data (one-hot encoded)</li></ul><p><strong>Returns</strong></p><ul><li><code>Float64</code>: Total loss across all training samples</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/rajgoel/course-machine-learning/blob/ea8c3bd49b4921d13be396e0667f546d7ad6e697/julia/02-lecture/Demo.jl#L74-L92">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MachineLearningCourse.Lecture02.ℒ-Tuple{Vector{Float64}, Vector{Float64}}" href="#MachineLearningCourse.Lecture02.ℒ-Tuple{Vector{Float64}, Vector{Float64}}"><code>MachineLearningCourse.Lecture02.ℒ</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">ℒ(y, â)</code></pre><p>Mean Squared Error loss function: ℒ = ‖â - y‖².</p><p><strong>Arguments</strong></p><ul><li><code>y::Vector{Float64}</code>: True target values</li><li><code>â::Vector{Float64}</code>: Computed values</li></ul><p><strong>Returns</strong></p><ul><li><code>Float64</code>: MSE loss value</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/rajgoel/course-machine-learning/blob/ea8c3bd49b4921d13be396e0667f546d7ad6e697/julia/02-lecture/GradientDescent.jl#L3-L14">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MachineLearningCourse.Lecture02.∂ℒ_∂â-Tuple{Vector{Float64}, Vector{Float64}}" href="#MachineLearningCourse.Lecture02.∂ℒ_∂â-Tuple{Vector{Float64}, Vector{Float64}}"><code>MachineLearningCourse.Lecture02.∂ℒ_∂â</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">∂ℒ_∂â(y, â)</code></pre><p>Gradient of MSE loss with respect to computed activations: ∂ℒ/∂â = 2(â - y).</p><p><strong>Arguments</strong></p><ul><li><code>y::Vector{Float64}</code>: True target values</li><li><code>â::Vector{Float64}</code>: Computed values</li></ul><p><strong>Returns</strong></p><ul><li><code>Vector{Float64}</code>: Gradient vector ∂ℒ/∂â</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/rajgoel/course-machine-learning/blob/ea8c3bd49b4921d13be396e0667f546d7ad6e697/julia/02-lecture/GradientDescent.jl#L20-L31">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MachineLearningCourse.Lecture03" href="#MachineLearningCourse.Lecture03"><code>MachineLearningCourse.Lecture03</code></a> — <span class="docstring-category">Module</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><p><strong>Lecture03: Feed forward networks</strong></p><p>A vanilla implementation of a deep neural network with fully connected layers, ReLU activation for all hidden layers, and MSE loss.</p><p><strong>Exported Functions</strong></p><ul><li><a href="#MachineLearningCourse.Lecture03.DNN"><code>DNN</code></a>: Constructor for deep neural network structure</li><li><a href="#MachineLearningCourse.Lecture03.train!"><code>train!</code></a>: Training function using backpropagation</li><li><a href="#MachineLearningCourse.Lecture03.predict-Tuple{MachineLearningCourse.Lecture03.DNN, Vector{Float64}}"><code>predict</code></a>: Prediction function for inference</li><li><a href="#MachineLearningCourse.Lecture03.demo"><code>demo</code></a>: MNIST handwritten digit recognition demonstration</li></ul><p><strong>Usage Examples</strong></p><p>Create and train a neural network:</p><pre><code class="language-julia hljs">using MachineLearningCourse.Lecture03

# Create network: 2 inputs → 4 hidden → 3 hidden → 1 output
network = DNN([2, 4, 3, 1])

# Train with learning rate 0.1 for 1000 epochs
losses = train!(network, X_train, Y_train, 0.1, 1000)

# Make predictions
prediction = predict(network, x_test)</code></pre><p>Run the MNIST demo:</p><pre><code class="language-julia hljs"># Default parameters (learning rate: 0.001, epochs: 50)
demo()

# Custom parameters
demo(0.005, 500)  # learning rate: 0.005, epochs: 500</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/rajgoel/course-machine-learning/blob/ea8c3bd49b4921d13be396e0667f546d7ad6e697/julia/03-lecture/Lecture03.jl#L1-L40">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MachineLearningCourse.Lecture03.DNN" href="#MachineLearningCourse.Lecture03.DNN"><code>MachineLearningCourse.Lecture03.DNN</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">DNN(layers)</code></pre><p>Deep Neural Network structure with fully connected layers.</p><p><strong>Arguments</strong></p><ul><li><code>layers::Vector{Int}</code>: Number of neurons per layer [input, hidden..., output]</li></ul><p><strong>Fields</strong></p><ul><li><code>layers::Vector{Int}</code>: Layer architecture specification</li><li><code>W::Vector{Matrix{Float64}}</code>: Weight matrices W^[l] for each layer</li><li><code>b::Vector{Vector{Float64}}</code>: Bias vectors b^[l] for each layer  </li><li><code>L::Int</code>: Total number of layers</li></ul><p>Uses He initialization for weights and zero initialization for biases. ReLU activation for hidden layers, linear activation for output layer.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs"># Create network: 784 inputs → 128 hidden → 64 hidden → 10 outputs
network = DNN([784, 128, 64, 10])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/rajgoel/course-machine-learning/blob/ea8c3bd49b4921d13be396e0667f546d7ad6e697/julia/03-lecture/DNN.jl#L11-L33">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MachineLearningCourse.Lecture03.backpropagation-Tuple{MachineLearningCourse.Lecture03.DNN, Vector{Vector{Float64}}, Vector{Vector{Float64}}, Vector{Float64}}" href="#MachineLearningCourse.Lecture03.backpropagation-Tuple{MachineLearningCourse.Lecture03.DNN, Vector{Vector{Float64}}, Vector{Vector{Float64}}, Vector{Float64}}"><code>MachineLearningCourse.Lecture03.backpropagation</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">backpropagation(network, activations, z_values, y)</code></pre><p>Compute gradients using backpropagation algorithm.</p><p>Calculates ∂ℒ/∂W^[l] and ∂ℒ/∂b^[l] for all layers using:</p><ul><li>∂ℒ/∂W^[l] = δ^[l] * (a^[l-1])^T  </li><li>∂ℒ/∂b^[l] = δ^[l]</li></ul><p><strong>Arguments</strong></p><ul><li><code>network::DNN</code>: Neural network structure</li><li><code>activations::Vector{Vector{Float64}}</code>: Layer activations from forward pass</li><li><code>z_values::Vector{Vector{Float64}}</code>: Linear combinations from forward pass</li><li><code>y::Vector{Float64}</code>: True target values</li></ul><p><strong>Returns</strong></p><ul><li><code>Tuple{Vector{Matrix{Float64}}, Vector{Vector{Float64}}}</code>: (∇W, ∇b)<ul><li><code>∇W</code>: Weight gradients for each layer</li><li><code>∇b</code>: Bias gradients for each layer</li></ul></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/rajgoel/course-machine-learning/blob/ea8c3bd49b4921d13be396e0667f546d7ad6e697/julia/03-lecture/DNN.jl#L141-L160">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MachineLearningCourse.Lecture03.demo" href="#MachineLearningCourse.Lecture03.demo"><code>MachineLearningCourse.Lecture03.demo</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">demo(learning_rate=0.001, epochs=50, seed=42)</code></pre><p>MNIST handwritten digit recognition demonstration.</p><p><strong>Parameters</strong></p><ul><li><code>learning_rate</code>: Learning rate for training (default: 0.001)</li><li><code>epochs</code>: Number of training epochs (default: 50)</li><li><code>seed</code>: Random seed (default: 42)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/rajgoel/course-machine-learning/blob/ea8c3bd49b4921d13be396e0667f546d7ad6e697/julia/03-lecture/Demo.jl#L155-L164">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MachineLearningCourse.Lecture03.evaluate_model-Tuple{MachineLearningCourse.Lecture03.DNN, Vector{Vector{Float64}}, Vector{Vector{Float64}}}" href="#MachineLearningCourse.Lecture03.evaluate_model-Tuple{MachineLearningCourse.Lecture03.DNN, Vector{Vector{Float64}}, Vector{Vector{Float64}}}"><code>MachineLearningCourse.Lecture03.evaluate_model</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">evaluate_model(network, X_test, Y_test)</code></pre><p>Evaluate neural network performance on test data and display detailed results.</p><p><strong>Arguments</strong></p><ul><li><code>network::DNN</code>: Trained neural network</li><li><code>X_test::Vector{Vector{Float64}}</code>: Test input data</li><li><code>Y_test::Vector{Vector{Float64}}</code>: Test target labels (one-hot encoded)</li></ul><p><strong>Returns</strong></p><ul><li><code>Float64</code>: Test accuracy (0.0 to 1.0)</li></ul><p>Prints comprehensive evaluation including per-digit accuracy, confusion matrix summary, and sample predictions.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/rajgoel/course-machine-learning/blob/ea8c3bd49b4921d13be396e0667f546d7ad6e697/julia/03-lecture/Demo.jl#L17-L31">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MachineLearningCourse.Lecture03.forwardpropagation-Tuple{MachineLearningCourse.Lecture03.DNN, Vector{Float64}}" href="#MachineLearningCourse.Lecture03.forwardpropagation-Tuple{MachineLearningCourse.Lecture03.DNN, Vector{Float64}}"><code>MachineLearningCourse.Lecture03.forwardpropagation</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">forwardpropagation(network, x)</code></pre><p>Compute forward propagation through the neural network.</p><p>Mathematical formulation:</p><ul><li>z^[l] = W^[l] * a^[l-1] + b^[l]</li><li>a^[l] = σ(z^[l]) for hidden layers, a^[l] = z^[l] for output layer</li></ul><p><strong>Arguments</strong></p><ul><li><code>network::DNN</code>: Neural network structure</li><li><code>x::Vector{Float64}</code>: Input vector</li></ul><p><strong>Returns</strong></p><ul><li><code>Tuple{Vector{Vector{Float64}}, Vector{Vector{Float64}}}</code>: (activations, z_values)<ul><li><code>activations</code>: [a^[0], a^[1], ..., a^[L-1]] - activations for each layer</li><li><code>z_values</code>: [z^[1], z^[2], ..., z^[L-1]] - linear combinations for each layer</li></ul></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/rajgoel/course-machine-learning/blob/ea8c3bd49b4921d13be396e0667f546d7ad6e697/julia/03-lecture/DNN.jl#L94-L111">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MachineLearningCourse.Lecture03.load_mnist_data" href="#MachineLearningCourse.Lecture03.load_mnist_data"><code>MachineLearningCourse.Lecture03.load_mnist_data</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">load_mnist_data(train_size=5000, test_size=1000)</code></pre><p>Load and preprocess MNIST handwritten digit dataset.</p><p><strong>Arguments</strong></p><ul><li><code>train_size::Int</code>: Number of training samples to use (default: 5000)</li><li><code>test_size::Int</code>: Number of test samples to use (default: 1000)</li></ul><p><strong>Returns</strong></p><ul><li><code>Tuple</code>: (X<em>train, Y</em>train, X<em>test, Y</em>test)<ul><li><code>X_train::Vector{Vector{Float64}}</code>: Training images (flattened and normalized)</li><li><code>Y_train::Vector{Vector{Float64}}</code>: Training labels (one-hot encoded)</li><li><code>X_test::Vector{Vector{Float64}}</code>: Test images (flattened and normalized)</li><li><code>Y_test::Vector{Vector{Float64}}</code>: Test labels (one-hot encoded)</li></ul></li></ul><p>Images are flattened from 28×28 to 784-dimensional vectors and normalized to [0,1]. Labels are one-hot encoded for 10-class classification (digits 0-9).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/rajgoel/course-machine-learning/blob/ea8c3bd49b4921d13be396e0667f546d7ad6e697/julia/03-lecture/Demo.jl#L85-L103">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MachineLearningCourse.Lecture03.predict-Tuple{MachineLearningCourse.Lecture03.DNN, Vector{Float64}}" href="#MachineLearningCourse.Lecture03.predict-Tuple{MachineLearningCourse.Lecture03.DNN, Vector{Float64}}"><code>MachineLearningCourse.Lecture03.predict</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">predict(network, x)</code></pre><p>Make predictions using the trained neural network.</p><p>Performs forward propagation to compute network output ŷ.</p><p><strong>Arguments</strong></p><ul><li><code>network::DNN</code>: Trained neural network</li><li><code>x::Vector{Float64}</code>: Input vector</li></ul><p><strong>Returns</strong></p><ul><li><code>Vector{Float64}</code>: Network predictions (output layer activations)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/rajgoel/course-machine-learning/blob/ea8c3bd49b4921d13be396e0667f546d7ad6e697/julia/03-lecture/DNN.jl#L271-L284">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MachineLearningCourse.Lecture03.train!" href="#MachineLearningCourse.Lecture03.train!"><code>MachineLearningCourse.Lecture03.train!</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">train!(network, X, Y, α=0.01, epochs=1000, verbose=true)</code></pre><p>Train the neural network using gradient descent.</p><p>Implements the complete training algorithm:</p><ol><li>Forward propagation</li><li>Loss computation  </li><li>Backpropagation</li><li>Parameter update</li></ol><p><strong>Arguments</strong></p><ul><li><code>network::DNN</code>: Neural network (modified in-place)</li><li><code>X::Vector{Vector{Float64}}</code>: Training input data</li><li><code>Y::Vector{Vector{Float64}}</code>: Training target data</li><li><code>α::Float64</code>: Learning rate (default: 0.01)</li><li><code>epochs::Int</code>: Number of training epochs (default: 1000)</li><li><code>verbose::Bool</code>: Print training progress (default: true)</li></ul><p><strong>Returns</strong></p><ul><li><code>Vector{Float64}</code>: Training losses for each epoch</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/rajgoel/course-machine-learning/blob/ea8c3bd49b4921d13be396e0667f546d7ad6e697/julia/03-lecture/DNN.jl#L213-L234">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MachineLearningCourse.Lecture03.update_parameters!-Tuple{MachineLearningCourse.Lecture03.DNN, Vector{Matrix{Float64}}, Vector{Vector{Float64}}, Float64}" href="#MachineLearningCourse.Lecture03.update_parameters!-Tuple{MachineLearningCourse.Lecture03.DNN, Vector{Matrix{Float64}}, Vector{Vector{Float64}}, Float64}"><code>MachineLearningCourse.Lecture03.update_parameters!</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">update_parameters!(network, ∇W, ∇b, α)</code></pre><p>Update network parameters using gradient descent.</p><p>Parameter updates:</p><ul><li>W^[l] ← W^[l] - α * ∂ℒ/∂W^[l]</li><li>b^[l] ← b^[l] - α * ∂ℒ/∂b^[l]</li></ul><p><strong>Arguments</strong></p><ul><li><code>network::DNN</code>: Neural network (modified in-place)</li><li><code>∇W::Vector{Matrix{Float64}}</code>: Weight gradients</li><li><code>∇b::Vector{Vector{Float64}}</code>: Bias gradients  </li><li><code>α::Float64</code>: Learning rate</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/rajgoel/course-machine-learning/blob/ea8c3bd49b4921d13be396e0667f546d7ad6e697/julia/03-lecture/DNN.jl#L186-L200">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MachineLearningCourse.Lecture03.σ-Tuple{Real}" href="#MachineLearningCourse.Lecture03.σ-Tuple{Real}"><code>MachineLearningCourse.Lecture03.σ</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">σ(z)</code></pre><p>ReLU activation function: σ(z) = max(0, z).</p><p><strong>Arguments</strong></p><ul><li><code>z::Real</code>: Input value</li></ul><p><strong>Returns</strong></p><ul><li><code>Float64</code>: Activated value (0.0 if z ≤ 0, z if z &gt; 0)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/rajgoel/course-machine-learning/blob/ea8c3bd49b4921d13be396e0667f546d7ad6e697/julia/03-lecture/DNN.jl#L63-L73">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MachineLearningCourse.Lecture03.∂σ_∂z-Tuple{Real}" href="#MachineLearningCourse.Lecture03.∂σ_∂z-Tuple{Real}"><code>MachineLearningCourse.Lecture03.∂σ_∂z</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">∂σ_∂z(z)</code></pre><p>Derivative of ReLU activation function: ∂σ/∂z = 1 if z &gt; 0, 0 if z ≤ 0.</p><p><strong>Arguments</strong></p><ul><li><code>z::Real</code>: Input value</li></ul><p><strong>Returns</strong></p><ul><li><code>Float64</code>: Derivative value (1.0 if z &gt; 0, 0.0 if z ≤ 0)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/rajgoel/course-machine-learning/blob/ea8c3bd49b4921d13be396e0667f546d7ad6e697/julia/03-lecture/DNN.jl#L79-L89">source</a></section></article></article><nav class="docs-footer"><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.14.1 on <span class="colophon-date" title="Saturday 23 August 2025 14:56">Saturday 23 August 2025</span>. Using Julia version 1.11.6.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
