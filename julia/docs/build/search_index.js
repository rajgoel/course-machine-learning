var documenterSearchIndex = {"docs":
[{"location":"lecture03/#Lecture-03:-Deep-Neural-Networks","page":"Lecture 03: Deep Neural Networks","title":"Lecture 03: Deep Neural Networks","text":"","category":"section"},{"location":"lecture03/","page":"Lecture 03: Deep Neural Networks","title":"Lecture 03: Deep Neural Networks","text":"This module implements a complete deep neural network from scratch in Julia, including:","category":"page"},{"location":"lecture03/","page":"Lecture 03: Deep Neural Networks","title":"Lecture 03: Deep Neural Networks","text":"Forward propagation\nBackpropagation\nGradient descent training\nMNIST digit classification demo","category":"page"},{"location":"lecture03/#Architecture","page":"Lecture 03: Deep Neural Networks","title":"Architecture","text":"","category":"section"},{"location":"lecture03/","page":"Lecture 03: Deep Neural Networks","title":"Lecture 03: Deep Neural Networks","text":"The implementation uses a flexible architecture where you can specify any number of layers:","category":"page"},{"location":"lecture03/","page":"Lecture 03: Deep Neural Networks","title":"Lecture 03: Deep Neural Networks","text":"# Example architectures:\nnetwork_small = DNN([2, 4, 1])           # Simple: 2 → 4 → 1\nnetwork_mnist = DNN([784, 128, 64, 10])  # MNIST: 784 → 128 → 64 → 10\nnetwork_deep = DNN([100, 50, 25, 10, 1]) # Deep: 100 → 50 → 25 → 10 → 1","category":"page"},{"location":"lecture03/#Key-Features","page":"Lecture 03: Deep Neural Networks","title":"Key Features","text":"","category":"section"},{"location":"lecture03/#Activation-Functions","page":"Lecture 03: Deep Neural Networks","title":"Activation Functions","text":"","category":"section"},{"location":"lecture03/","page":"Lecture 03: Deep Neural Networks","title":"Lecture 03: Deep Neural Networks","text":"Hidden layers: ReLU activation (σ(z) = max(0, z))\nOutput layer: Linear activation (for regression/raw scores)","category":"page"},{"location":"lecture03/#Loss-Function","page":"Lecture 03: Deep Neural Networks","title":"Loss Function","text":"","category":"section"},{"location":"lecture03/","page":"Lecture 03: Deep Neural Networks","title":"Lecture 03: Deep Neural Networks","text":"Mean Squared Error: ℒ = ||y - ŷ||²","category":"page"},{"location":"lecture03/#Initialization","page":"Lecture 03: Deep Neural Networks","title":"Initialization","text":"","category":"section"},{"location":"lecture03/","page":"Lecture 03: Deep Neural Networks","title":"Lecture 03: Deep Neural Networks","text":"Weights: He initialization (W ~ N(0, 2/n_in))\nBiases: Zero initialization","category":"page"},{"location":"lecture03/#Training-Algorithm","page":"Lecture 03: Deep Neural Networks","title":"Training Algorithm","text":"","category":"section"},{"location":"lecture03/","page":"Lecture 03: Deep Neural Networks","title":"Lecture 03: Deep Neural Networks","text":"Standard gradient descent\nConfigurable learning rate\nConfigurable number of epochs","category":"page"},{"location":"lecture03/#MNIST-Demo","page":"Lecture 03: Deep Neural Networks","title":"MNIST Demo","text":"","category":"section"},{"location":"lecture03/","page":"Lecture 03: Deep Neural Networks","title":"Lecture 03: Deep Neural Networks","text":"The demo showcases the neural network on the classic MNIST handwritten digit classification task:","category":"page"},{"location":"lecture03/","page":"Lecture 03: Deep Neural Networks","title":"Lecture 03: Deep Neural Networks","text":"demo()  # Run with defaults\ndemo(0.01, 100)  # Custom learning rate and epochs","category":"page"},{"location":"lecture03/#Demo-Process","page":"Lecture 03: Deep Neural Networks","title":"Demo Process","text":"","category":"section"},{"location":"lecture03/","page":"Lecture 03: Deep Neural Networks","title":"Lecture 03: Deep Neural Networks","text":"Data Loading: Downloads and preprocesses MNIST dataset\nPreprocessing: \nFlattens 28×28 images to 784-dimensional vectors\nNormalizes pixel values to [0, 1]\nOne-hot encodes labels for 10 digit classes\nTraining: Trains network using backpropagation\nEvaluation: \nComputes overall test accuracy\nShows per-digit classification performance\nDisplays sample predictions with confidence scores","category":"page"},{"location":"lecture03/#Expected-Performance","page":"Lecture 03: Deep Neural Networks","title":"Expected Performance","text":"","category":"section"},{"location":"lecture03/","page":"Lecture 03: Deep Neural Networks","title":"Lecture 03: Deep Neural Networks","text":"With default settings, the network typically achieves:","category":"page"},{"location":"lecture03/","page":"Lecture 03: Deep Neural Networks","title":"Lecture 03: Deep Neural Networks","text":"Training time: ~2-3 minutes\nTest accuracy: ~85-95% (depending on random initialization)\nPer-digit performance: Usually best on digits 0, 1, 6; more challenging on 4, 8, 9","category":"page"},{"location":"lecture03/#Mathematical-Background","page":"Lecture 03: Deep Neural Networks","title":"Mathematical Background","text":"","category":"section"},{"location":"lecture03/#Forward-Propagation","page":"Lecture 03: Deep Neural Networks","title":"Forward Propagation","text":"","category":"section"},{"location":"lecture03/","page":"Lecture 03: Deep Neural Networks","title":"Lecture 03: Deep Neural Networks","text":"For each layer l:","category":"page"},{"location":"lecture03/","page":"Lecture 03: Deep Neural Networks","title":"Lecture 03: Deep Neural Networks","text":"z^[l] = W^[l] * a^[l-1] + b^[l]\na^[l] = σ(z^[l])","category":"page"},{"location":"lecture03/#Backpropagation","page":"Lecture 03: Deep Neural Networks","title":"Backpropagation","text":"","category":"section"},{"location":"lecture03/","page":"Lecture 03: Deep Neural Networks","title":"Lecture 03: Deep Neural Networks","text":"Computing gradients via chain rule:","category":"page"},{"location":"lecture03/","page":"Lecture 03: Deep Neural Networks","title":"Lecture 03: Deep Neural Networks","text":"δ^[L] = ∂ℒ/∂a^[L]\nδ^[l] = (W^[l+1])^T * δ^[l+1] ⊙ σ'(z^[l])\n∂ℒ/∂W^[l] = δ^[l] * (a^[l-1])^T\n∂ℒ/∂b^[l] = δ^[l]","category":"page"},{"location":"lecture03/#Parameter-Updates","page":"Lecture 03: Deep Neural Networks","title":"Parameter Updates","text":"","category":"section"},{"location":"lecture03/","page":"Lecture 03: Deep Neural Networks","title":"Lecture 03: Deep Neural Networks","text":"Standard gradient descent:","category":"page"},{"location":"lecture03/","page":"Lecture 03: Deep Neural Networks","title":"Lecture 03: Deep Neural Networks","text":"W^[l] ← W^[l] - α * ∂ℒ/∂W^[l]\nb^[l] ← b^[l] - α * ∂ℒ/∂b^[l]","category":"page"},{"location":"#MachineLearningCourse.jl","page":"Home","title":"MachineLearningCourse.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"A Julia package for machine learning course materials and implementations.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The recommended way to use this package is to clone the repository","category":"page"},{"location":"","page":"Home","title":"Home","text":"git clone https://github.com/rajgoel/course-machine-learning","category":"page"},{"location":"","page":"Home","title":"Home","text":"and load the module as follows (replace path/to/ by the path to the repository):","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg\nPkg.develop(path=\"path/to/course-machine-learning/julia\")","category":"page"},{"location":"","page":"Home","title":"Home","text":"Alternatively, you can directly load the module as follows:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg\nPkg.develop(url=\"https://github.com/rajgoel/course-machine-learning\", subdir=\"julia\")","category":"page"},{"location":"#Usage","page":"Home","title":"Usage","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To use the module type:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using MachineLearningCourse","category":"page"},{"location":"","page":"Home","title":"Home","text":"To access specific lecture modules (replace XX with the two-digit lecture number):","category":"page"},{"location":"","page":"Home","title":"Home","text":"using MachineLearningCourse.LectureXX","category":"page"},{"location":"","page":"Home","title":"Home","text":"For example, to run the demos:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using MachineLearningCourse\n\n# Run Lecture 02 gradient descent demo\nMachineLearningCourse.Lecture02.demo()\n\n# Run Lecture 03 deep network demo\nMachineLearningCourse.Lecture03.demo()","category":"page"},{"location":"#Course-material","page":"Home","title":"Course material","text":"","category":"section"},{"location":"#MachineLearningCourse.MachineLearningCourse","page":"Home","title":"MachineLearningCourse.MachineLearningCourse","text":"MachineLearningCourse\n\nA Julia package for machine learning course materials and implementations.\n\nContent\n\nLecture02: Gradient descent\nLecture03: Feed forward networks\n\n\n\n\n\n","category":"module"},{"location":"#MachineLearningCourse.Lecture02","page":"Home","title":"MachineLearningCourse.Lecture02","text":"Lecture02: Gradient Descent\n\nA vanilla implementation of gradient descent.\n\nExported Functions\n\ndemo: 5x5 digit gradient descent demonstration\n\nUsage Examples\n\nCreate and train a neural network:\n\nusing MachineLearningCourse.Lecture03\n\n# Create network: 2 inputs → 4 hidden → 3 hidden → 1 output\nnetwork = DNN([2, 4, 3, 1])\n\n# Train with learning rate 0.1 for 1000 epochs\nlosses = train!(network, X_train, Y_train, 0.1, 1000)\n\n# Make predictions\nprediction = predict(network, x_test)\n\nRun the 5x5 digit demo:\n\ndemo()\n\n\n\n\n\n","category":"module"},{"location":"#MachineLearningCourse.Lecture02.compute_average_gradients-Tuple{Matrix{Float64}, Vector{Float64}, Vector{Vector{Float64}}, Vector{Vector{Float64}}}","page":"Home","title":"MachineLearningCourse.Lecture02.compute_average_gradients","text":"compute_average_gradients(W, b, X, Y)\n\nCompute average gradients across all training samples for batch gradient descent.\n\nPerforms gradient computation for each sample and averages the results:\n\nFor each sample (xi, yi): compute ∇Wi, ∇bi\nReturn average: (1/N) * Σ(∇Wi), (1/N) * Σ(∇bi)\n\nArguments\n\nW::Matrix{Float64}: Weight matrix (noutputs × ninputs)\nb::Vector{Float64}: Bias vector (n_outputs,)\nX::Vector{Vector{Float64}}: Training input data (N samples)\nY::Vector{Vector{Float64}}: Training target data (N samples)\n\nReturns\n\nTuple{Matrix{Float64}, Vector{Float64}}: (avg∇W, avg∇b)\navg_∇W: Average weight gradients\navg_∇b: Average bias gradients\n\n\n\n\n\n","category":"method"},{"location":"#MachineLearningCourse.Lecture02.compute_gradients-Tuple{Matrix{Float64}, Vector{Float64}, Vector{Float64}, Vector{Float64}}","page":"Home","title":"MachineLearningCourse.Lecture02.compute_gradients","text":"compute_gradients(W, b, x, y)\n\nCompute gradients ∂ℒ/∂W and ∂ℒ/∂b for a single sample using backpropagation.\n\nCalculates gradients using the chain rule:\n\n∂ℒ/∂W = δ * x^T where δ = ∂ℒ/∂â\n∂ℒ/∂b = δ\n\nArguments\n\nW::Matrix{Float64}: Weight matrix (noutputs × ninputs)\nb::Vector{Float64}: Bias vector (n_outputs,)\nx::Vector{Float64}: Input vector for single sample\ny::Vector{Float64}: Target output vector for single sample\n\nReturns\n\nTuple{Matrix{Float64}, Vector{Float64}}: (∇W, ∇b)\n∇W: Weight gradients (same size as W)\n∇b: Bias gradients (same size as b)\n\n\n\n\n\n","category":"method"},{"location":"#MachineLearningCourse.Lecture02.demo","page":"Home","title":"MachineLearningCourse.Lecture02.demo","text":"demo()\n\nDemonstration of gradient descent on 5x5 digit recognition.\n\nLoads training data, initializes a linear classifier, and optimizes parameters using gradient descent. Prints initial and final loss values.\n\nImplementation Details\n\nNetwork: Linear classifier (25 inputs → 10 outputs)\nLoss: Mean Squared Error\nOptimization: Batch gradient descent\nRandom initialization for weights and biases\n\nExample\n\ndemo()  # Uses sample data file\n\n\n\n\n\n","category":"function"},{"location":"#MachineLearningCourse.Lecture02.gradient_descent-Tuple{Matrix{Float64}, Vector{Float64}, Vector{Vector{Float64}}, Vector{Vector{Float64}}}","page":"Home","title":"MachineLearningCourse.Lecture02.gradient_descent","text":"gradient_descent(W_0, b_0, X, Y)\n\nOptimize neural network parameters using batch gradient descent.\n\nImplements the complete gradient descent algorithm:\n\nInitialize parameters from W0, b0\nFor each iteration:\nCompute average gradients across all samples\nUpdate parameters: W ← W - α * ∇W, b ← b - α * ∇b\nCheck convergence using gradient norm\nStop when ‖∇‖ < tolerance or max iterations reached\n\nArguments\n\nW_0::Matrix{Float64}: Initial weight matrix (noutputs × ninputs)\nb_0::Vector{Float64}: Initial bias vector (n_outputs,)\nX::Vector{Vector{Float64}}: Training input data\nY::Vector{Vector{Float64}}: Training target data (one-hot encoded)\n\nReturns\n\nTuple{Matrix{Float64}, Vector{Float64}}: (W, b)\nW: Optimized weight matrix\nb: Optimized bias vector\n\nImplementation Details\n\nLearning rate: α = 0.1\nConvergence tolerance: 1e-3\nMaximum iterations: 10,000\nUses MSE loss function: ℒ = ‖â - y‖²\n\n\n\n\n\n","category":"method"},{"location":"#MachineLearningCourse.Lecture02.gradient_norm-Tuple{Any, Any}","page":"Home","title":"MachineLearningCourse.Lecture02.gradient_norm","text":"gradient_norm(∇W, ∇b)\n\nCompute the Euclidean norm of the combined gradient vector.\n\nFlattens and concatenates weight and bias gradients into a single vector, then computes ‖∇‖ = √(‖∇W‖² + ‖∇b‖²) for convergence monitoring.\n\nArguments\n\n∇W::Matrix{Float64}: Weight gradients\n∇b::Vector{Float64}: Bias gradients\n\nReturns\n\nFloat64: Euclidean norm of the combined gradient vector\n\n\n\n\n\n","category":"method"},{"location":"#MachineLearningCourse.Lecture02.one_hot_encode-Tuple{Int64, Int64}","page":"Home","title":"MachineLearningCourse.Lecture02.one_hot_encode","text":"one_hot_encode(label, num_classes)\n\nConvert class labels to one-hot vectors for classification.\n\nArguments\n\nlabel::Int: Class label (1-indexed)\nnum_classes::Int: Total number of classes\n\nReturns\n\nVector{Float64}: One-hot encoded vector\n\nExample\n\none_hot_encode(3, 10)  # Returns [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n\n\n\n\n\n","category":"method"},{"location":"#MachineLearningCourse.Lecture02.read_data-Tuple{Any}","page":"Home","title":"MachineLearningCourse.Lecture02.read_data","text":"read_data(file_path)\n\nRead 5x5 digit training data from a text file.\n\nFile format: Each digit consists of 6 lines:\n\n5 lines of 5 space-separated Float64 values (5x5 pixel grid)\n1 line with the digit label (0-9)\n\nArguments\n\nfile_path::String: Path to the data file\n\nReturns\n\nTuple{Vector{Vector{Float64}}, Vector{Vector{Float64}}}: (X, Y)\nX: Input vectors (each vector has 25 elements from 5x5 grid)\nY: One-hot encoded target vectors (10 classes, 1-indexed)\n\nExample\n\nX, Y = read_data(\"5x5digits.txt\")\n# X[1] contains 25 pixel values for first digit\n# Y[1] contains one-hot vector for first digit's class\n\n\n\n\n\n","category":"method"},{"location":"#MachineLearningCourse.Lecture02.total_loss-Tuple{Matrix{Float64}, Vector{Float64}, Vector{Vector{Float64}}, Vector{Vector{Float64}}}","page":"Home","title":"MachineLearningCourse.Lecture02.total_loss","text":"total_loss(W, b, X, Y)\n\nCompute total Mean Squared Error loss across all training samples.\n\nFor each sample, performs forward pass and computes loss:\n\nForward pass: â = W * x + b\nSample loss: ℒ(y, â) = ‖â - y‖²\nTotal loss: Σ ℒ(yi, âi) over all samples\n\nArguments\n\nW::Matrix{Float64}: Weight matrix (noutputs × ninputs)\nb::Vector{Float64}: Bias vector (n_outputs,)\nX::Vector{Vector{Float64}}: Training input data\nY::Vector{Vector{Float64}}: Training target data (one-hot encoded)\n\nReturns\n\nFloat64: Total loss across all training samples\n\n\n\n\n\n","category":"method"},{"location":"#MachineLearningCourse.Lecture02.ℒ-Tuple{Vector{Float64}, Vector{Float64}}","page":"Home","title":"MachineLearningCourse.Lecture02.ℒ","text":"ℒ(y, â)\n\nMean Squared Error loss function: ℒ = ‖â - y‖².\n\nArguments\n\ny::Vector{Float64}: True target values\nâ::Vector{Float64}: Computed values\n\nReturns\n\nFloat64: MSE loss value\n\n\n\n\n\n","category":"method"},{"location":"#MachineLearningCourse.Lecture02.∂ℒ_∂â-Tuple{Vector{Float64}, Vector{Float64}}","page":"Home","title":"MachineLearningCourse.Lecture02.∂ℒ_∂â","text":"∂ℒ_∂â(y, â)\n\nGradient of MSE loss with respect to computed activations: ∂ℒ/∂â = 2(â - y).\n\nArguments\n\ny::Vector{Float64}: True target values\nâ::Vector{Float64}: Computed values\n\nReturns\n\nVector{Float64}: Gradient vector ∂ℒ/∂â\n\n\n\n\n\n","category":"method"},{"location":"#MachineLearningCourse.Lecture03","page":"Home","title":"MachineLearningCourse.Lecture03","text":"Lecture03: Feed forward networks\n\nA vanilla implementation of a deep neural network with fully connected layers, ReLU activation for all hidden layers, and MSE loss.\n\nExported Functions\n\nDNN: Constructor for deep neural network structure\ntrain!: Training function using backpropagation\npredict: Prediction function for inference\ndemo: MNIST handwritten digit recognition demonstration\n\nUsage Examples\n\nCreate and train a neural network:\n\nusing MachineLearningCourse.Lecture03\n\n# Create network: 2 inputs → 4 hidden → 3 hidden → 1 output\nnetwork = DNN([2, 4, 3, 1])\n\n# Train with learning rate 0.1 for 1000 epochs\nlosses = train!(network, X_train, Y_train, 0.1, 1000)\n\n# Make predictions\nprediction = predict(network, x_test)\n\nRun the MNIST demo:\n\n# Default parameters (learning rate: 0.001, epochs: 50)\ndemo()\n\n# Custom parameters\ndemo(0.005, 500)  # learning rate: 0.005, epochs: 500\n\n\n\n\n\n","category":"module"},{"location":"#MachineLearningCourse.Lecture03.DNN","page":"Home","title":"MachineLearningCourse.Lecture03.DNN","text":"DNN(layers)\n\nDeep Neural Network structure with fully connected layers.\n\nArguments\n\nlayers::Vector{Int}: Number of neurons per layer [input, hidden..., output]\n\nFields\n\nlayers::Vector{Int}: Layer architecture specification\nW::Vector{Matrix{Float64}}: Weight matrices W^[l] for each layer\nb::Vector{Vector{Float64}}: Bias vectors b^[l] for each layer  \nL::Int: Total number of layers\n\nUses He initialization for weights and zero initialization for biases. ReLU activation for hidden layers, linear activation for output layer.\n\nExample\n\n# Create network: 784 inputs → 128 hidden → 64 hidden → 10 outputs\nnetwork = DNN([784, 128, 64, 10])\n\n\n\n\n\n","category":"type"},{"location":"#MachineLearningCourse.Lecture03.backpropagation-Tuple{MachineLearningCourse.Lecture03.DNN, Vector{Vector{Float64}}, Vector{Vector{Float64}}, Vector{Float64}}","page":"Home","title":"MachineLearningCourse.Lecture03.backpropagation","text":"backpropagation(network, activations, z_values, y)\n\nCompute gradients using backpropagation algorithm.\n\nCalculates ∂ℒ/∂W^[l] and ∂ℒ/∂b^[l] for all layers using:\n\n∂ℒ/∂W^[l] = δ^[l] * (a^[l-1])^T  \n∂ℒ/∂b^[l] = δ^[l]\n\nArguments\n\nnetwork::DNN: Neural network structure\nactivations::Vector{Vector{Float64}}: Layer activations from forward pass\nz_values::Vector{Vector{Float64}}: Linear combinations from forward pass\ny::Vector{Float64}: True target values\n\nReturns\n\nTuple{Vector{Matrix{Float64}}, Vector{Vector{Float64}}}: (∇W, ∇b)\n∇W: Weight gradients for each layer\n∇b: Bias gradients for each layer\n\n\n\n\n\n","category":"method"},{"location":"#MachineLearningCourse.Lecture03.demo","page":"Home","title":"MachineLearningCourse.Lecture03.demo","text":"demo(learning_rate=0.001, epochs=50, seed=42)\n\nMNIST handwritten digit recognition demonstration.\n\nParameters\n\nlearning_rate: Learning rate for training (default: 0.001)\nepochs: Number of training epochs (default: 50)\nseed: Random seed (default: 42)\n\n\n\n\n\n","category":"function"},{"location":"#MachineLearningCourse.Lecture03.evaluate_model-Tuple{MachineLearningCourse.Lecture03.DNN, Vector{Vector{Float64}}, Vector{Vector{Float64}}}","page":"Home","title":"MachineLearningCourse.Lecture03.evaluate_model","text":"evaluate_model(network, X_test, Y_test)\n\nEvaluate neural network performance on test data and display detailed results.\n\nArguments\n\nnetwork::DNN: Trained neural network\nX_test::Vector{Vector{Float64}}: Test input data\nY_test::Vector{Vector{Float64}}: Test target labels (one-hot encoded)\n\nReturns\n\nFloat64: Test accuracy (0.0 to 1.0)\n\nPrints comprehensive evaluation including per-digit accuracy, confusion matrix summary, and sample predictions.\n\n\n\n\n\n","category":"method"},{"location":"#MachineLearningCourse.Lecture03.forwardpropagation-Tuple{MachineLearningCourse.Lecture03.DNN, Vector{Float64}}","page":"Home","title":"MachineLearningCourse.Lecture03.forwardpropagation","text":"forwardpropagation(network, x)\n\nCompute forward propagation through the neural network.\n\nMathematical formulation:\n\nz^[l] = W^[l] * a^[l-1] + b^[l]\na^[l] = σ(z^[l]) for hidden layers, a^[l] = z^[l] for output layer\n\nArguments\n\nnetwork::DNN: Neural network structure\nx::Vector{Float64}: Input vector\n\nReturns\n\nTuple{Vector{Vector{Float64}}, Vector{Vector{Float64}}}: (activations, z_values)\nactivations: [a^[0], a^[1], ..., a^[L-1]] - activations for each layer\nz_values: [z^[1], z^[2], ..., z^[L-1]] - linear combinations for each layer\n\n\n\n\n\n","category":"method"},{"location":"#MachineLearningCourse.Lecture03.load_mnist_data","page":"Home","title":"MachineLearningCourse.Lecture03.load_mnist_data","text":"load_mnist_data(train_size=5000, test_size=1000)\n\nLoad and preprocess MNIST handwritten digit dataset.\n\nArguments\n\ntrain_size::Int: Number of training samples to use (default: 5000)\ntest_size::Int: Number of test samples to use (default: 1000)\n\nReturns\n\nTuple: (Xtrain, Ytrain, Xtest, Ytest)\nX_train::Vector{Vector{Float64}}: Training images (flattened and normalized)\nY_train::Vector{Vector{Float64}}: Training labels (one-hot encoded)\nX_test::Vector{Vector{Float64}}: Test images (flattened and normalized)\nY_test::Vector{Vector{Float64}}: Test labels (one-hot encoded)\n\nImages are flattened from 28×28 to 784-dimensional vectors and normalized to [0,1]. Labels are one-hot encoded for 10-class classification (digits 0-9).\n\n\n\n\n\n","category":"function"},{"location":"#MachineLearningCourse.Lecture03.predict-Tuple{MachineLearningCourse.Lecture03.DNN, Vector{Float64}}","page":"Home","title":"MachineLearningCourse.Lecture03.predict","text":"predict(network, x)\n\nMake predictions using the trained neural network.\n\nPerforms forward propagation to compute network output ŷ.\n\nArguments\n\nnetwork::DNN: Trained neural network\nx::Vector{Float64}: Input vector\n\nReturns\n\nVector{Float64}: Network predictions (output layer activations)\n\n\n\n\n\n","category":"method"},{"location":"#MachineLearningCourse.Lecture03.train!","page":"Home","title":"MachineLearningCourse.Lecture03.train!","text":"train!(network, X, Y, α=0.01, epochs=1000, verbose=true)\n\nTrain the neural network using gradient descent.\n\nImplements the complete training algorithm:\n\nForward propagation\nLoss computation  \nBackpropagation\nParameter update\n\nArguments\n\nnetwork::DNN: Neural network (modified in-place)\nX::Vector{Vector{Float64}}: Training input data\nY::Vector{Vector{Float64}}: Training target data\nα::Float64: Learning rate (default: 0.01)\nepochs::Int: Number of training epochs (default: 1000)\nverbose::Bool: Print training progress (default: true)\n\nReturns\n\nVector{Float64}: Training losses for each epoch\n\n\n\n\n\n","category":"function"},{"location":"#MachineLearningCourse.Lecture03.update_parameters!-Tuple{MachineLearningCourse.Lecture03.DNN, Vector{Matrix{Float64}}, Vector{Vector{Float64}}, Float64}","page":"Home","title":"MachineLearningCourse.Lecture03.update_parameters!","text":"update_parameters!(network, ∇W, ∇b, α)\n\nUpdate network parameters using gradient descent.\n\nParameter updates:\n\nW^[l] ← W^[l] - α * ∂ℒ/∂W^[l]\nb^[l] ← b^[l] - α * ∂ℒ/∂b^[l]\n\nArguments\n\nnetwork::DNN: Neural network (modified in-place)\n∇W::Vector{Matrix{Float64}}: Weight gradients\n∇b::Vector{Vector{Float64}}: Bias gradients  \nα::Float64: Learning rate\n\n\n\n\n\n","category":"method"},{"location":"#MachineLearningCourse.Lecture03.σ-Tuple{Real}","page":"Home","title":"MachineLearningCourse.Lecture03.σ","text":"σ(z)\n\nReLU activation function: σ(z) = max(0, z).\n\nArguments\n\nz::Real: Input value\n\nReturns\n\nFloat64: Activated value (0.0 if z ≤ 0, z if z > 0)\n\n\n\n\n\n","category":"method"},{"location":"#MachineLearningCourse.Lecture03.∂σ_∂z-Tuple{Real}","page":"Home","title":"MachineLearningCourse.Lecture03.∂σ_∂z","text":"∂σ_∂z(z)\n\nDerivative of ReLU activation function: ∂σ/∂z = 1 if z > 0, 0 if z ≤ 0.\n\nArguments\n\nz::Real: Input value\n\nReturns\n\nFloat64: Derivative value (1.0 if z > 0, 0.0 if z ≤ 0)\n\n\n\n\n\n","category":"method"}]
}
